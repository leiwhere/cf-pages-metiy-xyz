* [在一张 24 GB 的消费级显卡上用 RLHF 微调 20B LLMs](https://huggingface.co/blog/zh/trl-peft)
* [大语言模型快速推理：在 Habana Gaudi2 上推理 BLOOMZ](https://huggingface.co/blog/zh/habana-gaudi-2-bloom)
* [StackLLaMA：使用 RLHF 训练 LLaMA 的实践指南](https://hub.baai.ac.cn/view/25341)
* [大模型训练实战](https://techdiylife.github.io/)
* [RLHF 实践教程: 训练 LLaMA 模型回答 Stack Exchange 上的问题 作者：HuggingFace](https://huggingface.co/blog/stackllama)
* [使用 LoRA 和 Hugging Face 高效训练大语言模型](https://www.bilibili.com/read/cv23019960)
* [Hugging News #0428: HuggingChat 来啦！](https://www.bilibili.com/read/cv23388841)
* [使用 DeepSpeed 和 Hugging Face Transformer 微调 FLAN-T5 XL/XXL](https://www.bilibili.com/read/cv22542818)
* [2.7B 的代码补全模型](https://huggingface.co/replit/replit-code-v1-3b)
* [OpenLLama 项目，LLama模型的开源复现](https://huggingface.co/openlm-research/open_llama_7b_preview_200bt)
* [闻达：一个大规模语言模型调用平台](https://github.com/l15y/wenda)
* [发布几个RWKV的Chat模型（包括英文和中文）7B/14B欢迎大家玩](https://zhuanlan.zhihu.com/p/618011122)
* [当你有100万GPU小时，如何选择合适的语言模型](arXiv:2210.15424）
* [标注 bigscience-workshop/prompt source](https://github.com/bigscience-workshop/promptsource)
* [star coder huggingface  playground](https://huggingface.co/spaces/bigcode/bigcode-playground)
* [star coder endpoint server](https://github.com/LucienShui/huggingface-vscode-endpoint-server)
* [huggingface chat](https://huggingface.co/chat/conversation/645b1072d422882b4eac42eb)
* [OpenAI API兼容项目](https://github.com/hyperonym/basaran)
* [开源 LLMOps 平台](https://www.oschina.net/news/241075/dify-open-source)
