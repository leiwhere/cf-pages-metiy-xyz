* 多智能体/推理/对照/联合决策
* 模型中毒
* 投毒
* 幻觉
* 遗忘
* 自监督
* prompt
* 奖赏模型
* chain of thought 思维链
* 预训练参数 权重
* 优化器
* 注意力机制 attention
* 注意头
* 多头注意力
* token 编码 解码
* 编码器 解码器
* 训练
* 微调
* LORA
* finetune
* transformer
* zero-shot
* few-shot
* Chinchilla Scaling Law 金吉拉规则，对于大语言模型，充分训练所需的语料经验值为模型参数的20～50倍（ 20 tokens per param ）[参考](https://lifearchitect.ai/chinchilla/#:~:text=In%20Sep%2F2022%2C%20DeepMind%20%28Chinchilla%20paper%29%20found%20new%20data,train%20a%20data-optimal%20LLM%20of%20size%2070B%20parameters)
* 灾难性遗忘，二次训练模型时会发生模型在预训练过的任务表现极具变差的情况，这种现象倍称作灾难性遗忘
